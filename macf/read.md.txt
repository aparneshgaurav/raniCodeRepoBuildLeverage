cachedModelEditor
#sparksql 
#dsalgo
#sparkCore
#kafka
#lessons
#python
#scala





#lessons

kafka 
	how to read in ordered method from distributed consumers 
sql
	joins 
fileformats
	avro vs orc 
delta 
	transactional logs
comparsions
	databricks ingestion 
	databricks vs snowflake 
deployment 
	spark mlib models
	mlFlow



#lessons

tempTodayTasks incremental percetage completion for 25thjuly is : 1 %

rough model editor 

Test$100^user


kjadsf


asdflkjasd
f


temp
rdds are immutable jvm objects , which are meant to be transformed suing programming languages . 

spark session from session builder

df from hive table , rdd or other spark based sources like json or text or soemthing 

datafarme operations totally resembble to query language , so better to be a master of spark dataframe query style , so that it can be helpful in other tools like snowflake as well . 

keep taking notes , then restructure . 



#dsAlgo



#dsAlgo



#sparkSql



temp
#sparksql 


#sparkSql

string functions are of two types , one is that will return a single value , like count or sum .. 

the other type will mutate teh column and return the column with mutated values , 


this is something where practice woiuld be needed .. 

you will need to do it on databricks editor 

returning unix timestamp : 

df = df.select( unixTimeStamp( col ( timeStamp1 ).alias("time1")  , unixTimeStamp ( col( timeStamp2 ) , "dd mm yy hh mm ss ") . alias("time2")






#sparkSql



#dsalgo
#dsalgo

in non linear search   for searching you iterate the entire array and try to do the match . 

In sorted array , you keep matching , but if the arary value gets bigger than the value to be matched , in that case , you return -1 

binary search , applicable to searching in sorted data , 

while( low is less than high) 
{
mid = low plus high by 2 
if data is equal to mid , found . 
else if data is less than mid , 
then , high = mid - 1
else if data is more than mid , 
then low = mid + 1

}

check duplicates 

for int i=0 ; i++ , 
for j=i+2; j+++,
if a of i is equal to a of j , 
then duplicate 
and so on .. 

if we improve it , then , 
for int i=0;i++
if a of i is equal to a of i+1

check if in array if two elements sum is equal to element k 
for i -0;i++
for j=i+1;j++

if a of i plus a of j is equal to k . 

Similar above goes for three elements whose sum is equal to k . 
Just that j is i plus 1 , and k is j plus 1 . 



#dsalgo


#python



#python



-----

thislist = [ "apple" , "bat" ]

for x in thislist:
  print(x)

thislist.reverse()



thisdict  = {
"x" : "xmas"
"y" : "year"
}

car = {
"brand": "Ford",
"model": "Mustang",
"year": 1964
}

x = car.keys()

for y in x:
  print(y)
  print(car.get(y))



def my_function(parameter):
  print(" function getting executed with this parameter " + parameter)

my_function("Emil")
my_function("Tobias")
my_function("Linus")



 function getting executed with this parameter Emil
 function getting executed with this parameter Tobias
 function getting executed with this parameter Linus


lambdaAsyncExpressionFunctionName = lambda a, b: a * b
print(lambdaAsyncExpressionFunctionName(5, 6))

output is 
30 

#sparksql 


import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()

arrayData = [
        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),
        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),
        ('Robert',['CSharp',''],{'hair':'red','eye':''}),
        ('Washington',None,None),
        ('Jefferson',['1','2'],{})

df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])
df.printSchema()
df.show()




+----------+--------------+--------------------+
|      name|knownLanguages|          properties|
+----------+--------------+--------------------+
|     James| [Java, Scala]|[eye -> brown, ha...|
|   Michael|[Spark, Java,]|[eye ->, hair -> ...|
|    Robert|    [CSharp, ]|[eye -> , hair ->...|
|Washington|          null|                null|
| Jefferson|        [1, 2]|                  []|
+----------+--------------+--------------------+


df2 = df.select( df.name , explode ( df.knowLanguages)
df2.printSchema()
df2.show()


+---------+------+
|     name|   col|
+---------+------+
|    James|  Java|
|    James| Scala|
|  Michael| Spark|
|  Michael|  Java|
|  Michael|  null|
|   Robert|CSharp|
|   Robert|      |
|Jefferson|     1|
|Jefferson|     2|

df3 = df.select( df.name , explode ( df.properties) 
df3.printShecma()
df3.show()

+-------+----+-----+
|   name| key|value|
+-------+----+-----+
|  James| eye|brown|
|  James|hair|black|
|Michael| eye| null|
|Michael|hair|brown|
| Robert| eye|     |
| Robert|hair|  red|

lambda is expression based manipulation . 



+---------+--------+------+------+
|firstname|lastname|gender|salary|
+---------+--------+------+------+
|    James|   Smith|     M|    30|
|     Anna|    Rose|     F|    41|
|   Robert|Williams|     M|    62|
+---------+--------+------+------+

# Refering columns by index.
rdd2=df.rdd.map(lambda x: 
    (x[0]+","+x[1],x[2],x[3]*2)
    )  
df2=rdd2.toDF(["name","gender","new_salary"]   )
df2.show()
+---------------+------+----------+
|           name|gender|new_salary|
+---------------+------+----------+
|    James,Smith|     M|        60|
|      Anna,Rose|     F|        82|
|Robert,Williams|     M|       124|


rdd2=rdd.map(lambda x: (x,1))
for element in rdd2.collect():
    print(element)
This yields below output.

pyspark rdd map transformation

apple,1
bat,1
apple,1
batter,1
and so on ... 


#sparkmlib
mainly it's estimator and then transformer . 
It's a series and  combination of those . 
trasnformer is an algorithm which takes dataframe as a parameter in i'ts fit method and produces a model . 

that model when applided on the dataframe products anotehr dataframe , whic is a predicted dtatframe with appended column / label which is the predicted column . This step is transformation . 

parameters , estimators and and transfsformers have parameters . 
like LR has setIterator(10) values so it iterates 10 tiiems before producing a model .

models are saved on hard disck using save methods . 

https://spark.apache.org/docs/latest/ml-pipeline.html#properties-of-pipeline-components 

further read the model and it's examples from this page . aboev page . 

spark streaming model deployment is pending 
spark mlib code sample is pending 
different algos read is pending 

-----