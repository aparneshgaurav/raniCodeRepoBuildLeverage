cachedModelEditor
#sparksql 
#dsalgo
#sparkCore
#kafka
#lessons



#lessons

kafka 
	how to read in ordered method from distributed consumers 
sql
	joins 
fileformats
	avro vs orc 
delta 
	transactional logs
comparsions
	databricks ingestion 
	databricks vs snowflake 
deployment 
	spark mlib models
	mlFlow



#lessons

tempTodayTasks incremental percetage completion for 25thjuly is : 1 %

rough model editor 

Test$100^user


kjadsf


asdflkjasd
f


temp
rdds are immutable jvm objects , which are meant to be transformed suing programming languages . 

spark session from session builder

df from hive table , rdd or other spark based sources like json or text or soemthing 

datafarme operations totally resembble to query language , so better to be a master of spark dataframe query style , so that it can be helpful in other tools like snowflake as well . 

keep taking notes , then restructure . 



#dsAlgo

fibonacci(n) { 

if n==0 {
return 1 
}
else{
return  n*fibonacci(n-1)
}

}


control list , rest is same . 
do stack via list 
do queue via list 
do double list via list 
do tree via list 
list plus recursion is all it takes 

public LinkedList{

LinkedList next ; 
Integer data ; 

pubilc LinkedList(data){
this.data = data ; 
}

set method for data 
get method for data 
set method for list 
get method for list 

}

insert(head , n,data ) {
temp=head ; 
n=0
while(temp.next!= null ) {
n++;
if( m==n){
nextNode = temp.next ; 
newNode = new LinkedList(data ) ; 
temp.next = newNode; 
newNode.next = nextNode ; 

}

}




package dsAlgo;

public class SingleLinkedList {
	
	
	Integer data ; 
	SingleLinkedList next ; 
	
	public SingleLinkedList(Integer data) {
		this.data = data ; 
	}

	public Integer getData() {
		return data;
	}

	public void setData(Integer data) {
		this.data = data;
	}

	public SingleLinkedList getNext() {
		return next;
	}

	public void setNext(SingleLinkedList next) {
		this.next = next;
	}
	
	
	
	public void traverseNodes(SingleLinkedList head) {
		SingleLinkedList temp = head ; 
		while((temp.getNext()!=null) || (temp.getNext()==null)) {
			System.out.println("data is : "+temp.getData());
			if(temp!=null) {
			temp = temp.getNext();
			}
			if(temp==null) {
				break;
			}
//			System.out.println("data is : "+temp.getData());
		}
	}
	
	public static void main(String[] args) {
		SingleLinkedList newNode = new SingleLinkedList(10);
		SingleLinkedList nextNode = new SingleLinkedList(20);
		newNode.setNext(nextNode);
		nextNode.setNext(null);
		newNode.traverseNodes(newNode);		
		
		
	}

}


/*
 * output is
 * 
 * data is : 10 data is : 20
 */


#dsAlgo



#sparkSql



temp
#sparksql 
make table from dataframe 
df.createOrReplaceTempView("tableNameLikePeople")
sqlDataframe = spark.sql(" select * from people " ) 
sqlDataframe.show()

for across sessions, register it in common database , 
spark.newSession().sql("select * from global_temp.people") 

no need to understand more of lambda here , you can understand lambda separately as well . 

#sparkSql

temp


first we generate dataframes in saprk 

then we can have spark sql being fired upon those spark dataframes . 

dataframes generation can be done by using data  source formats like csv , parquet , avro , json , xml , tab separated .

then we create table view from those dataframes and then have spark sql queries on those temp tables and views . 

df = spark.read .csv or spark.read.json or spark.read.txt (  file path ) , it's like this .. 

also other route is to have create temp views and tables without dataframes generation 

spark.read.csv.header(true).csv( path to the file ) .createTempView('tablename")

spark.sql("select col1 from tableName ") 

where works exactly like any ql syntax .. 

select from where col in () order by col 

select state , count(star ) from zipcodes group by state having count greater than something 




#sparkSql