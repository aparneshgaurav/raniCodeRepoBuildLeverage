

data platforms architecture : 
https://docs.google.com/document/d/1VJJoeWyqO-Htv2nP7oJv2iaKlDHdSLI4xm0VdqsMal4/edit


	when you say data platforms 
		you must also know reporting 
		you must also know ml 
		you must also know ingestion and compute 
		all of it 
		databases
		database modelling
		no sqls 
		comparions of cloud vendors
----------

The best tech for each task:

- batch pipeline: Apache Spark
- data visualization: Apache Superset
- web api: NextJS (spring boot close second)
- SQL database: Postgres
- NoSQL database: DynamoDB
- Graph database: Neo4j
- front end web: React
- front end mobile: React Native (Flutter close second)
- CI/CD system: GitHub Actions
- data quality checks: Great Expectations (Deequ close second)
- data lake file management: Apache Iceberg (Delta Lake a close second)
- job orchestration: Apache Airflow (Mage and/or Prefect close second)
- machine learning model: XGBoost (linear regression close second)
- LLM: GPT-4.5 Turbo
- programming language: Python (Rust close second)
- message queue: Kafka (RabbitMQ close second)
- cache: Redis (Memcached close second)

		--------------------

data engineering tools 

https://docs.google.com/document/d/1EbuL-73DGN-DPSzTKj3PkX-8HX7vPDHMchhdceOjgiI/edit?disco=AAABDhKN_qQ


# some ml skills 
How do you rate yourself in Evaluation of LLMs, grounding and hallucination detection on a scale of 1-5? (5 being the highest)How do you rate yourself in Evaluation of LLMs, grounding and hallucination detection on a scale of 1-5? (5 being the highest) 
Required
4 (Above average)
How do you rate yourself in linear ML algorithms and non-linear ML algorithms on a scale of 1-5? (5 being the highest)How do you rate yourself in linear ML algorithms and non-linear ML algorithms on a scale of 1-5? (5 being the highest) 
Required
4 (Above average)
How do you rate yourself in RAG, Search algorithms, and Multi Lingual Modelling on a scale of 1-5? (5 being the highest)How do you rate yourself in RAG, Search algorithms, and Multi Lingual Modelling on a scale of 1-5? (5 being the highest) 
Required
5 (Expert)
How do you rate yourself in Model deployments, monitoring and AutoML frameworks & MLOps frameworks on a scale of 1-5? (5 being the highest)How do you rate yourself in Model deployments, monitoring and AutoML frameworks & MLOps frameworks on a scale of 1-5? (5 being the highest) 
Required
4 (Above average)
How do you rate yourself in Model measurement frameworks on a scale of 1-5? (5 being the highest)How do you rate yourself in Model measurement frameworks on a scale of 1-5? (5 being the highest) 
Required
4 (Above average) 
How do you rate yourself in Mathematical and Statistical modelling on a scale of 1-5? (5 being the highest)How do you rate yourself in Mathematical and Statistical modelling on a scale of 1-5? (5 being the highest) 
Required
3 (Average)
How do you rate yourself in using NLP algorithms such as keyword extraction, topic modelling using transformer models like BERT, T5, BART on a scale of 1-5? (5 being the highest)How do you rate yourself in using NLP algorithms such as keyword extraction, topic modelling using transformer models like BERT, T5, BART on a scale of 1-5? (5 being the highest) 
Required
4 (Above average)
How do you rate yourself in evaluating the performance of classification and regression models on a scale of 1-5? (5 being the highest)How do you rate yourself in evaluating the performance of classification and regression models on a scale of 1-5? (5 being the highest) 
Required
3 (Average)
How do you rate yourself in Python on a scale of 1-5? (5 being the highest)How do you rate yourself in Python on a scale of 1-5? (5 being the highest) 
Required
3 (Average)
How do you rate yourself in multivariate and statistical analysis on a scale of 1-5? (5 being the highest)How do you rate yourself in multivariate and statistical analysis on a scale of 1-5? (5 being the highest) 
Required
Select an option
Please make a selection
How do you rate yourself in Time Series and anomaly detection algorithms on a scale of 1-5? (5 being the highest)How do you rate yourself in Time Series and anomaly detection algorithms on a scale of 1-5? (5 being the highest) 
Required
Select an option
Please make a selection
How do you rate yourself in unsupervised learning (specifically, k-means clustering and hierarchical clustering) on a scale of 1-5? (5 being the highest)How do you rate yourself in unsupervised learning (specifically, k-means clustering and hierarchical clustering) on a scale of 1-5? (5 being the highest) 
Required
Select an option
Please make a selection
Please indicate your last CTC
Please enter a valid answer
Please indicate your last RSU
Please enter a valid answer
Please indicate your nationality
Please enter a valid answer
Please indicate your citizenship
Please enter a valid answer
Do you hold a valid work permit for the country where the job is located?Do you hold a valid work permit for the country where the job is located? Required

Yes

No
Please make a selection
Have you been previously employed with Freshworks?Have you been previously employed with Freshworks? Required

Yes

No
Please make a selection
Your message to the hiring manager

--------------------------------------------------------------------
	#sql
		Have two tables . Two dimension tables like POjo tables / classes of java . Primary keys of the rows of those dimension tables would be like object's hashcodes of the objects corresponding to those POJO classes . 
		Then there are facts tables which are only having details of the object's hashvalues of both the pojo classes . 
		create view viewName as select 
		When you do the join , you first based on pj1 and FC based on a common id , you query other stuffs and create a view out of that . That other id which you queried remains as a primary key in the pj2 , so based on the other common id now , you query the other remaning columns . 
		You need to see how group by happens . secondary group by would happen . 

   #cdc on data platforms 
   	just a matter of no large number of small files . 
   	also not to have so much large files that lambda fifteen minutes sla fails.
   	so bin packing or having those lambda holders shoudl be configured accordingly . 
   #columnarDatabase
   	each column  a file 
   	those files can be on separate computers 
   	columns in same column family can have adjacency and can be atleast on same / nearby machines . 
   	metadata files know which columns are on which machines and which offsets have been queried . They are the brains . 
   #catalog
   	#pushToROne
   	##hashtag
   	differential changes are being put to kafka . 
   	then from there to delta lake . 
   	Then to snowflake 
   	where the metadata tables are designed . 
	you can talk design of the atlas here . 
-------------------------------------------------------
some good preparation materials : 
	https://www.turing.com/interview-questions/machine-learning
	https://www.turing.com/interview-questions/data-engineer
	https://www.turing.com/interview-questions/spark
-------------------------------------------------------------   
# Read the input file and Calculating words count
text_file = sc.textFile("firstprogram.txt")
counts = text_file.flatMap(lambda line: line.split(" ")) \
                            .map(lambda word: (word, 1)) \
                           .reduceByKey(lambda x, y: x + y)

Note that here "text_file" is a RDD and we used "map", "flatmap", "reducebykey" transformations

Finally, initiate an action to collect the final result and print. Use the below snippet to do it and Here collect is an action that we used to gather the required output.

# Printing each word with its respective count
output = counts.collect()
for (word, count) in output:
    print("%s: %i" % (word, count))

--------------------------------------------

hbase architecture

hbase master 
region servers
regions 

hbbase masters coordinate the region servers . 
region servers contain the regions . 
regions loook like the jvm or the task trackers , if the region servers are the nodes . 

while writing .
we write to the region . 
then we write to Write ahead log files , which is write buffer layer . 
Then the buffer writes to hfile / flushes to hfile . 

while writing , it reads from block cache which contains frequently retrived data . this is at a layer above the hfile . 

there are two types of tables , root table and meta table . these tables are at the regionservers and contains the list of all the regions which contain the queried data . 

the way data is stored in a region is that , one column family , in colum oriented fasion  is stored in one region till space of 256mb . 
so the frequently queried columns , due to being in same column family can be found co-locally in regions . So given the set of column families that needs to be queried and if we know the range / offsets , then that data can be queried from those region / regions . 

Distributed and Scalable:
Column-oriented Storage: 
Hadoop Integration: 
Consistency and Replication:
Built-in Caching: 
Compression:
Flexible Schema:

#hbase
-----------------------------------------------
#pyspark
data = [1, 2, 3, 4, 5];
rdd1=sc.parallelize(data);
squared_rdd = rdd1.map(lambda x: x**2);
result = squared_rdd.collect();
print(result); 

---------------------------------------------------
#snowflake
columnar

   c1 c2 c3 c4 
r1 a1
r2 a2
r3 a3
r4 a4

microPartition1
   r1 r2 
c1 a1 a2
c2
c3
c4

microPartition2
   r3 r4 
c1 a3 a4
c2
c3
c4

so if we need column1 values for row 3 and row4 
	in that we need to traverse microPartition2 file . 
	this is more performant . 
