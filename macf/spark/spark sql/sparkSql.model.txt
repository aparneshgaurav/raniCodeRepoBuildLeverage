

temp , diff here is increasing the font size . 
#sparksql 
make table from dataframe 
df.createOrReplaceTempView("tableNameLikePeople")
sqlDataframe = spark.sql(" select * from people " ) 
sqlDataframe.show()

for across sessions, register it in common database , 
spark.newSession().sql("select * from global_temp.people") 

no need to understand more of lambda here , you can understand lambda separately as well . 

#sparkSql

temp


first we generate dataframes in saprk 

then we can have spark sql being fired upon those spark dataframes . 

dataframes generation can be done by using data  source formats like csv , parquet , avro , json , xml , tab separated .

then we create table view from those dataframes and then have spark sql queries on those temp tables and views . 

df = spark.read .csv or spark.read.json or spark.read.txt (  file path ) , it's like this .. 

also other route is to have create temp views and tables without dataframes generation 

spark.read.csv.header(true).csv( path to the file ) .createTempView('tablename")

spark.sql("select col1 from tableName ") 

where works exactly like any ql syntax .. 

select from where col in () order by col 

select state , count(star ) from zipcodes group by state having count greater than something 





import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()

arrayData = [
        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),
        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),
        ('Robert',['CSharp',''],{'hair':'red','eye':''}),
        ('Washington',None,None),
        ('Jefferson',['1','2'],{})

df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])
df.printSchema()
df.show()




+----------+--------------+--------------------+
|      name|knownLanguages|          properties|
+----------+--------------+--------------------+
|     James| [Java, Scala]|[eye -> brown, ha...|
|   Michael|[Spark, Java,]|[eye ->, hair -> ...|
|    Robert|    [CSharp, ]|[eye -> , hair ->...|
|Washington|          null|                null|
| Jefferson|        [1, 2]|                  []|
+----------+--------------+--------------------+


df2 = df.select( df.name , explode ( df.knowLanguages)
df2.printSchema()
df2.show()


+---------+------+
|     name|   col|
+---------+------+
|    James|  Java|
|    James| Scala|
|  Michael| Spark|
|  Michael|  Java|
|  Michael|  null|
|   Robert|CSharp|
|   Robert|      |
|Jefferson|     1|
|Jefferson|     2|

df3 = df.select( df.name , explode ( df.properties) 
df3.printShecma()
df3.show()

+-------+----+-----+
|   name| key|value|
+-------+----+-----+
|  James| eye|brown|
|  James|hair|black|
|Michael| eye| null|
|Michael|hair|brown|
| Robert| eye|     |
| Robert|hair|  red|

lambda is expression based manipulation . 



+---------+--------+------+------+
|firstname|lastname|gender|salary|
+---------+--------+------+------+
|    James|   Smith|     M|    30|
|     Anna|    Rose|     F|    41|
|   Robert|Williams|     M|    62|
+---------+--------+------+------+

# Refering columns by index.
rdd2=df.rdd.map(lambda x: 
    (x[0]+","+x[1],x[2],x[3]*2)
    )  
df2=rdd2.toDF(["name","gender","new_salary"]   )
df2.show()
+---------------+------+----------+
|           name|gender|new_salary|
+---------------+------+----------+
|    James,Smith|     M|        60|
|      Anna,Rose|     F|        82|
|Robert,Williams|     M|       124|


rdd2=rdd.map(lambda x: (x,1))
for element in rdd2.collect():
    print(element)
This yields below output.

pyspark rdd map transformation

apple,1
bat,1
apple,1
batter,1
and so on ... 


---------------------------------------------
---------------------------------------------

create table dpgTechnologies ( technology varchar ) 
# this will create the table with one column of type varchar

create table dpgUsers( emailid varchar , technology varchar , paidStatus boolean ) 
# this will create the table with three columns with type varchar and boolean
alter table dpgusers add column ( dpgUserId varchar )
# altered table to add  a column , while adding we have to give name and datatype 
alter table dpgusers drop column technology;
#altered the table , while dropping column , we just need to give the column name . 
alter table dpgUsers drop column paidstatus
# same as above .
insert into dpgusers values  ( 'gaurav@gmail.com' , '1')
# insertion via values syntax , which has to be given values as the parameter , strings to be put under comma 
insert into dpgusers values  ( 'khushi@gmail.com' , '1')

insert into dpgtechnologies values ( 'java')

insert into dpgtechnologies values ( 'spark')

insert into dpgtechnologies values ( 'jquery')

alter table dpgtechnologies add column ( technologyid varchar ) 

update  dpgtechnologies set technologyid='1' where technology = 'java'

update  dpgtechnologies set technologyid='2' where technology = 'spark'

update  dpgtechnologies set technologyid='2' where technology = 'jquery'

insert into dpgusers values ('rani' , '3')

insert into dpgusers values ('anku' , '4')

insert into dpgusers values ('banku' , '5')

insert into dpgusers values ('vidhu' , '6')

alter table dpgusers alter column dpguserid set not null

alter table dpgusers add primary key (dpguserid)

delete from dpgusers where dpguserid = '6' 

create table factsOfdpgusersandtechnologies ( dpguserid varchar ,  technolgoies varchar ) 

drop table factsofdpgusersandtechnologies

create table factsOfdpgusersandtechnologies ( dpguserid varchar ,  technologyid varchar ) 

insert into factsofdpgusersandtechnologies values ( '2' , '1' ) 

insert into factsofdpgusersandtechnologies values ( '3' , '2' ) 

insert into factsofdpgusersandtechnologies values ( '4' , '1' ) 

insert into factsofdpgusersandtechnologies values ( '5' , '2' ) 

insert into factsofdpgusersandtechnologies values ( '5' , '1' ) 

select * from factsofdpgusersandtechnologies 

 ( select emailid from dpgusers where dpguserid in ( select dpguserid from factsofdpgusersandtechnologies where technologyid = '2' )  ) 
EMAILID  
rani
banku

select emailid , technologyid  from dpgusers  inner join factsofdpgusersandtechnologies  on dpgusers.dpguserid =  factsofdpgusersandtechnologies.dpguserid  where technologyid='2'

EMAILID  	TECHNOLOGYID  
rani	2
banku	2

create table emailTech ( emailid varchar  , technologyid varchar ) 

insert into emailtech ( select emailid , technologyid  from dpgusers  inner join factsofdpgusersandtechnologies  on dpgusers.dpguserid =  factsofdpgusersandtechnologies.dpguserid  where technologyid='2' )

alter table emailtech rename  to emailidtechid
EMAILID  	TECHNOLOGYID  
rani	2
banku	2


select emailid , technology from dpgtechnologies inner join v1 on v1.technologyid = dpgtechnologies.technologyid 
EMAILID  	TECHNOLOGY  
rani	jquery
banku	jquery

--- nested join --- three joins together via view 

create view v2 as select emailid  , technologyid from dpgusers inner join factsofdpgusersandtechnologies on dpgusers.dpguserid = factsofdpgusersandtechnologies.dpguserid where technologyid ='2' 
result is view created 
( Here you joined one fact talbe and one dimnesion table on the common id and also did put a condition of where from either of the tables , in this particular case it was from the facts table for a technology id two )

select emailid , technology from dpgtechnologies inner join v2 on v2.technologyid = dpgtechnologies.technologyid
( in the view2 which was created in the preceeding statement , we did a join on this view with a dimension table dpgtechnologies on the common id which is technologyid , and after that fetched other columns like email id and technolgy where ever there was a match / inner join for the common technology id  )

EMAILID  	TECHNOLOGY  
rani	jquery
banku	jquery

-------------------------------------

select upper(emailid) , technologyid from v2 limit 1 


UPPER(EMAILID)  	TECHNOLOGYID  
RANI	2

select top 1 upper(emailid) , technologyid from v2 order by emailid asc
UPPER(EMAILID)  	TECHNOLOGYID  
BANKU	2

select top 1 upper(emailid) , technologyid from v2 order by emailid desc
UPPER(EMAILID)  	TECHNOLOGYID  
RANI	2

 alter table dpgusers add column ( city varchar ) 


update dpgusers set city = 'delhi' 


update dpgusers set city = 'patna' where emailid = 'rani'

select * from dpgusers 

EMAILID  	DPGUSERID  	CITY  
anku	4	delhi
banku	5	delhi
rani	3	patna

select count(dpguserid ) , city from dpgusers  group by city having count(dpguserid ) >1 order by city asc
COUNT(DPGUSERID)  	CITY  
2	delhi

when doing a group by , in that case the column on which you are diong group by , should also be fetched as a parameter . Also any aggregation shouldn't happen on the column on which you are doing group by , rather it should happen on one of the other columns . 
--------------------------------------

select city  , RANK() OVER (    
    PARTITION BY city
    ORDER BY dpgusers desc )    
AS 'useridRankingPerCity' FROM dpgusers;

result : 
this will give the ranking and ranks of each dpguser ranked on dpguserid within each city . 
rank function in my sql server isn't installed . 

----------------------------------------
----------------------------------------
----------------------------------------
------------------------------------------------------------------------------------------------------------------------
create table netflix ( deviceid varchar , accountid varchar , date varchar ) 

insert into netflix values ( 'd4' , 'a1' , 'jun' ) 

 create view view8 as (   select count(deviceid ) as cdcount , accountid from netflix group by accountid )

select count(accountid ) , cdcount  from   view8 group by cdcount 

COUNT(ACCOUNTID)  	CDCOUNT  
1	11
1	5

-----------

query of run selected mode : 

drop view view8 if exists ;
 create view view8 as (   select count(deviceid ) as cdcount , accountid from netflix group by accountid );
select count(accountid ) , cdcount  from   view8 group by cdcount ;

output : 
drop view view8 if exists ;
Update count: 0
(0 ms)

 create view view8 as (   select count(deviceid ) as cdcount , accountid from netflix group by accountid );
Update count: 0
(2 ms)

select count(accountid ) , cdcount  from   view8 group by cdcount ;
COUNT(ACCOUNTID)  	CDCOUNT  
2	2
1	11
1	5


###############################################################################################################################################

#dbvisualizer
#end of dbvisualizer

select * from dpgUsers
gaurav@gmail.com           1
rani                       3
anku                       4
banku                      5

select * from dpgTechnologies
java        1
spark       2

delete from dpgTechnologies where technology='jquery'
show tables 

select * from factsOfdpgusersandtechnologies
3       2
4       1
5       2
5       1
select * from dpgUsers
delete from dpgUsers where emailid='khushi@gmail.com'
select * from dpgUsers

select * from dpgUsers where dpgUserId='1'
gaurav@gmail.com        1

create view viewOfUsersUsingTechnologies as select dpgUsers.emailid  ,  factsOfdpgusersandtechnologies.technologyid from dpgUsers inner join factsOfdpgusersandtechnologies on dpgUsers.dpguserid = factsOfdpgusersandtechnologies.dpguserid ;
create view emailidAndTechnology as select dpgTechnologies.technology , viewOfUsersUsingTechnologies.emailid from viewOfUsersUsingTechnologies inner join dpgTechnologies on viewOfUsersUsingTechnologies.technologyid = dpgTechnologies.technologyid

# so in general we query col1 from table1 and col2 from table 2 
# when we inner join both the columns 
# on the basis of a third id like table1.col3=table2.col3 . Also explode 
# is very useful in cases like this . 
# also one should have a good grasp on pre existing aggregate functions . 


select * from dpgUsers
select * from dpgTechnologies 

select * from emailidAndTechnology
spark   rani
java    anku
java    banku
spark   banku


select count(technology), emailid from emailidAndTechnology group by emailid 
1       rani
1       anku
2       banku

# you group by one , aggregate on other column . and then select the aggregate(otherCol) and groupedBy col 

select count(technology), emailid from emailidAndTechnology group by emailid having count(technology) >1
2        banku


select emailid from dpgUsers where dpguserid in ( select dpguserid from factsOfdpgusersandtechnologies where technologyid='2' )
rani
banku

 
select emailid from dpgUsers where dpguserid in ( select dpguserid from factsOfdpgusersandtechnologies where technologyid='1' )
anku
banku

 
select emailid from dpgUsers where dpguserid 
        in  ( select dpguserid from factsOfdpgusersandtechnologies where technologyid 
        in  ( select technologyid from dpgTechnologies where technology='spark'))
rani
banku


select emailid from dpgUsers where dpguserid
        in ( select dpguserid from factsOfdpgusersandtechnologies where technologyid
        in ( select technologyid from dpgTechnologies where technology='java'))
anku
banku


select emailid , RANK () over ( order by emailid desc ) as  rankid from dpgUsers where dpguserid in ( select dpguserid from factsOfdpgusersandtechnologies where technologyid in ( select technologyid from dpgTechnologies where technology='java'))
banku        1
anku         2



select emailid , RANK () over ( order by emailid asc ) as  rankid from dpgUsers where dpguserid in ( select dpguserid from factsOfdpgusersandtechnologies where technologyid in ( select technologyid from dpgTechnologies where technology='java'))
create view rankView as select emailid , RANK () over ( order by emailid asc ) as  rankid from dpgUsers where dpguserid in ( select dpguserid from factsOfdpgusersandtechnologies where technologyid in ( select technologyid from dpgTechnologies where technology='java'))


select * from rankView where rankid='2'
banku        2

select * from ccc.factsOfdpgusersandtechnologies;
select * from ccc.factsOfdpgusersandtechnologies;


SELECT * from  ccc.dpgUsers
select * from ccc.dpgTechnologies
select * from ccc.factsOfdpgusersandtechnologies
delete from ccc.factsOfdpgusersandtechnologies where  dpguserid='2'
select * from ccc.dpgUsers
select * from ccc.factsOfdpgusersandtechnologies

show tables 
select * from thoughtsbookdb.login
delete from thoughtsbookdb.login where username = 'ramuser#gmail.com'

select username,urldata from thoughtsbookdb.login where length(username) > 7  and urldata not like '%data:image%' 



show tables 
CREATE TABLE FILESAVE( USERNAME VARCHAR(255), FILENAME VARCHAR(255), CODEFIELD VARCHAR(510) PRIMARY KEY, FILECONTENT TEXT );
CREATE TABLE SAMPLEFILES( FILEID VARCHAR(510) PRIMARY KEY, CODECONTENT TEXT);
ALTER TABLE FILESAVE ADD NUMBEROFCODERUNS INT DEFAULT 0;
create table chatbox(userid varchar(550) primary key, commentstring TEXT);

----------------------------------------------------------------------------------------

select * from thoughtsbookdb.SAMPLEFILES
----------------------------------------------------------------------------------------
show tables 
create table empRdcc ( name varchar(255) , city varchar (255) , lastName varchar(255) , language varchar(255))
insert into ccc.empRdcc VALUES('ram1' , 'patna' , 'srivastava' , 'python')
insert into ccc.empRdcc VALUES('ram2' , 'delhi' , 'yadav' , 'java')
insert into ccc.empRdcc VALUES('ram3' , 'patna' , 'kishor' , 'sql')
insert into ccc.empRdcc VALUES('shyam' , 'patna' , 'srivastava' , 'python')

select count(name) , city , language from ccc.empRdcc group by city , language
3        patna   python
1        delhi   java
1        patna   sql

select * from empRdcc
ram     patna   srivastava      python
ram1    patna   srivastava      python
ram2    delhi   yadav   java
ram3    patna   kishor  sql


create table empRdccSalary( name varchar(255) , days int , rate int ) 
insert into ccc.empRdccSalary values ( 'ram' , 20 , 100 ,  ) 
insert into ccc.empRdccSalary values ( 'shyam' , 30 , 100)
insert into ccc.empRdccSalary values ( 'mohan' , 50 , 500)
select name , days * rate as volume from ccc.empRdccSalary
ram     2000
shyam   3000
mohan   25000
ram     2000


alter table ccc.empRdccSalary add city varchar(255)
insert into ccc.empRdccSalary (city) values ('patna' )
update ccc.empRdccSalary set city='patna' where name='ram'
update ccc.empRdccSalary set city='patna' where name='shyam'
update ccc.empRdccSalary set city='delhi' where name='mohan'

select * from empRdccSalary
ram     20      100     patna   java
shyam   30      100     patna   python
mohan   50      500     delhi   python
ram     20      100     patna   java


delete from ccc.empRdccSalary where name is null 
select * from empRdccSalary
alter table ccc.empRdccSalary add technology varchar(255)
update ccc.empRdccSalary set technology='java' where name='ram'
update ccc.empRdccSalary set technology='python' where name='shyam'
update ccc.empRdccSalary set technology='python' where name='mohan'
#need to work on this addition of primary key 
alter TABLE ccc.empRdccSalary add PRIMARY key empRdccSalary.name

insert into ccc.empRdccSalary values ( 'ram' , 20 , 100 , 'patna','java' ) 
insert into ccc.empRdccSalary values ( 'shyam' , 30 , 100 ,'delhi', 'python');
insert into ccc.empRdccSalary values ( 'mohan' , 50 , 500 ,'delhi','python') 
select sum(days*rate) as totalIncome  ,  city  from empRdccSalary group by empRdccSalary.city 
select sum(days*rate) as totalIncome  ,  city  from empRdccSalary group by empRdccSalary.city 
select * from ccc.empRdccSalary


select sum(days*rate) as totalIncome  ,  city  from empRdccSalary group by empRdccSalary.city
delete from ccc.empRdccSalary where name='shyam' and city='delhi' 
create view paymentPerCityPerTechnology as select sum(days*rate) as totalIncome  ,  city , technology  from empRdccSalary group by empRdccSalary.city , technology
select city , technology from paymentPerCityPerTechnology

#the combination of the city and technology that got paid the highest
#group by 1 or group secondary by 2 
#aggreate on 3rd or 4th 
#order by aggregated columns 
select sum(days*rate) as totalIncome  ,  city , technology  from empRdccSalary
group by empRdccSalary.city , technology 
order by totalIncome desc
50000   delhi   python
4000    patna   java
3000    patna   python

#the city which took the maximum payment
select sum(days*rate) as totalIncome  ,  city  from empRdccSalary
group by empRdccSalary.city
order by totalIncome desc
50000   delhi
7000    patna


#the technology which took the maximum payment
select sum(days*rate) as totalIncome  , technology  from empRdccSalary 
group by technology 
order by totalIncome desc
53000   python
4000    java

#the programmer which took the maximum payment
select sum(days*rate) as totalIncome  , name   , rank() over (order by sum(days*rate) desc ) as pRank
from empRdccSalary group by name  
50000   mohan   1
4000    ram     2
3000    shyam   3

#the programmer which took the maximum payment
select sum(days*rate) as totalIncome  , name   , rank() over (order by sum(days*rate) asc ) as pRank
from empRdccSalary group by name  
3000    shyam   1
4000    ram     2
50000   mohan   3

create table datePoc(dates DATE);
insert into datePoc values ( '2022-01-21')
insert into datePoc values ( '2022-02-21')
select * from datePoc

select year(dates) from datePoc
2022
2022

select month(dates) from datePoc
1
2

select  date(dates) from datePoc
alter table datePoc add column sales integer;
update  datePoc set sales=1000 where year(dates)='2022'


select * from datePoc
2022-01-21      1000
2022-02-21      1000


# we use table keyword only in ddl , not in dml apparently 
select sum(sales) , year(dates) from datePoc group by year(dates)
2000        2022


select sum(sales) as totalSales , 
RANK () over (order by sum(sales) asc) as salesRanking  , 
month(dates) from datePoc 
group by month(dates)
totalSales salesRanking month(dates)
1000      1       1
1000      1       2


create table valueRanges(valuesHere integer[5])


#end of dbvisualizer
###########################################################
###########################################################
--------------------------------------------------------------------
pyspark codes on vm , vmware , pyspark , cloudera 

data = [1, 2, 3, 4, 5];
rdd1=sc.parallelize(data);
squared_rdd = rdd1.map(lambda x: x**2);
result = squared_rdd.collect();
print(result); 
-------------------------
output 
[1, 4, 9, 16, 25]
------------------------------



























































































